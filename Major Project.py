# -*- coding: utf-8 -*-
"""main_project_final_copy.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1U9eeRC2Sqo4kYJu8a9Po5W2lNezKQhIX

#**Pattern Recognition and Machine Learning (CSN2050) - 2022**
##**Major Project**

###**Stroke Prediction using Machine Learning**

### Dataset link: 
https://www.kaggle.com/datasets/fedesoriano/stroke-prediction-dataset
"""

# To Disable Convergence Warnings (For Custom Training)
from warnings import simplefilter
from sklearn.exceptions import ConvergenceWarning
simplefilter("ignore", category=ConvergenceWarning)

# Import the necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt 
import seaborn as sns
from scipy.stats import reciprocal, uniform
from imblearn.over_sampling import SMOTE
from collections import Counter
from sklearn.model_selection import train_test_split

# Encoder for encoding categorical values 
from sklearn.preprocessing import LabelEncoder

# Scalers for scaling the dataset
from sklearn.preprocessing import MinMaxScaler as MMS
from sklearn.preprocessing import StandardScaler as SS

# Metrics for performance evaluation
from sklearn.metrics import accuracy_score as accuracy
from sklearn.metrics import f1_score as f1score
from sklearn.metrics import precision_score as precision
from sklearn.metrics import recall_score as recall
from sklearn.metrics import confusion_matrix as cm
from sklearn.metrics import roc_auc_score
from sklearn.metrics import plot_roc_curve

# Importing the Classifiers
from sklearn.ensemble import RandomForestClassifier as RFC 
from sklearn.neighbors import KNeighborsClassifier as KNC
from sklearn.svm import LinearSVC as SVC
from xgboost import XGBClassifier as XGBC
from sklearn.neural_network import MLPClassifier as MLPC
from sklearn.neighbors import KNeighborsClassifier as KNC

# RandomizedSearchCV to find optimal hyperparameters
from sklearn.model_selection import RandomizedSearchCV

"""##**Preprocessing the Dataset**"""

# Reading the dataframe
df = pd.read_csv("/content/healthcare-dataset-stroke-data.csv")

# Printing the top rows of the dataframe
df.head()

# An overall description of the data
df.describe().T

# Encoding the columns with string values using Label Encoder to convert them into numerical
# values for easy computation.

label_encoder = LabelEncoder()
df['gender'] = label_encoder.fit_transform(df['gender'])
df['ever_married'] = label_encoder.fit_transform(df['ever_married'])
df['work_type'] = label_encoder.fit_transform(df['work_type'])
df['Residence_type'] = label_encoder.fit_transform(df['Residence_type'])
df['smoking_status'] = label_encoder.fit_transform(df['smoking_status'])

# Dimensions of dataset
df.shape

# Observing Null values
df.isnull().sum()

# Replacing the Null values by the mean of their respective columns
df.bmi.fillna(df.bmi.mean(),inplace = True)

# Distribution of data points across different attributes
cat_features = ["gender", "ever_married", "work_type", "Residence_type", "smoking_status","hypertension"]
for i in cat_features:
  print(i,":")
  for j in np.unique(np.array(df[i])):
    print(j,":",list(df[i]).count(j))

df.drop(df[df['gender'] == 2].index, axis = 0, inplace=True)

"""## **Visualizing the distribution of Data**"""

# Histograms showing distribution across the features
df.iloc[:,1:-1].hist(bins=50, figsize=(20,15))
plt.show()

# Heatmap to visualize the correlation between the attributes 
sns.heatmap(df[["age", 'work_type', 'avg_glucose_level', 'bmi', 'smoking_status']].corr(), vmin=-1, vmax=1, annot=True)

# X and y data
X = df.iloc[:,:-1]
y = df.iloc[:,-1]
del X['id']

"""## **Applying SMOTE to increase the samples containing 1 as predicted values**

This is done as the given dataset is highly skewed towards output label 0. This leads to the model being biased towards predictions with label 0, and it classifies predictions with output 1 as also 0, which is type - 2 error, and is highly critical here (Patients vulnerable to stroke are classified as the opposite which could prove to be deadly). Hence, we increase the number of samples with output value 1, so at the cost of accuracy we decrease the type - 2 error, which is a safer option in the world of diagnosis.
"""

# Applying SMOTE to increase the number of samples of output class y = 1
# to ensure uniform distribution of data points so that model does not
# lead to more type - 2 Error which is critical here
smote = SMOTE()
counter = Counter(y)
print('Original dataset shape', counter)

# fit predictor and target variable
X_after_smote, y_after_smote = smote.fit_resample(X, y)

counter = Counter(y_after_smote)
print('Resample dataset shape', counter)

"""Splitting the dataset ito train, validation and test data"""

# Splitting the data into train and test splits
X_train, X_test, y_train, y_test = train_test_split(X_after_smote, y_after_smote, stratify = y_after_smote, train_size=0.8, random_state=42)

# Using Min-Max Scaler to scale the data
scaler = MMS()
X_train['bmi'] = scaler.fit_transform(pd.DataFrame(X_train['bmi']))
X_train['avg_glucose_level'] = scaler.fit_transform(pd.DataFrame(X_train['avg_glucose_level']))
X_train['age'] = scaler.fit_transform(pd.DataFrame(X_train['age']))

# Splitting the data into train and validation splits
X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, stratify = y_train, train_size = 0.8, random_state = 42)

df1 = df.copy()

# Function to create stacked barcharts for better visualization of continuous features
def stacked_barchart(column):
  df_breakdown = df1.groupby([column,'stroke'])['age'].count()
  df_tot = df1.groupby([column])['age'].count()
  df_pct = (df_breakdown/df_tot)*100
  df_pct = df_pct.unstack()
  return df_pct.plot.bar(stacked=True,figsize=(5,5),width=0.5) 

# discretizing the continuous features
df1['age'] = pd.cut(df1['age'],np.arange(0,85,5))
df1['bmi'] = pd.cut(df1['bmi'],np.arange(10,100,5))
df1['avg_glucose_level'] = pd.cut(df1['avg_glucose_level'],np.arange(55,275,10))

"""Visualization of the distribution of features using stacked barcharts"""

# Stacked bar charts for visualization
for i in df1.columns[1:-1]:
  stacked_barchart(i)

# Function to calculate type-2 error (an important evaluation metrics here)
def type2_error(cm):
  return (cm[0][1]/(cm[0][1] + cm[1][1]))



"""# **Manual Hyperparameter tuning**

Here, we apply manual tuning of hyperparameters and check their performance over the validation dataset using the evaluation metrics as accuracy and type - 2 error.

### **1. Random Forest Classifier**
"""

model1 = RFC()
model1.fit(X_train,y_train)
y_pred1 = model1.predict(X_test)
accuracy(y_test,y_pred1),type2_error(cm(y_test,y_pred1))

for i in range(10):
  model1 = RFC(n_estimators = 100 + 50*i,max_depth = 2+i,min_samples_split = 2+ 2*i,max_features = 'auto',min_samples_leaf = 2+2*i)
  model1.fit(X_train,y_train)
  y_pred1 = model1.predict(X_validation)
  print("i = ",i)
  print(accuracy(y_validation,y_pred1),type2_error(cm(y_validation,y_pred1)))

min_samples_split = [2,3,5,7,10,15,20]
min_samples_leaf = [2,3,5,7,10,15,20]
for i in min_samples_leaf:
  for j in min_samples_split :
    model1 = RFC(n_estimators = 500,max_depth = 10,min_samples_split = j,max_features = 'auto',min_samples_leaf = i)
    model1.fit(X_train,y_train)
    y_pred1 = model1.predict(X_validation)
    print("i = ",i,"j = ",j)
    print(accuracy(y_validation,y_pred1),type2_error(cm(y_validation,y_pred1)))

n_estimators = [200,250,300,350,400,450,500,550,600,650,700]
max_depth = [2,3,5,7,10]
for i in n_estimators:
  for j in max_depth :
    model1 = RFC(n_estimators = i,max_depth = j,min_samples_split = 2,max_features = 'auto',min_samples_leaf = 2)
    model1.fit(X_train,y_train)
    y_pred1 = model1.predict(X_validation)
    print("i = ",i,"j = ",j)
    print(accuracy(y_validation,y_pred1),type2_error(cm(y_validation,y_pred1)))

model1 = RFC(n_estimators = 600,max_depth = 10,min_samples_split = 2,max_features = 'auto',min_samples_leaf = 2)
model1.fit(X_train,y_train)
y_pred1 = model1.predict_proba(X_test)
y_pred11 = model1.predict(X_test)
#  print("i = ",i,"j = ",j)
print(roc_auc_score(y_test,y_pred1[:,1]),type2_error(cm(y_test,y_pred11)))

"""### **2. Support Vector Classifier**"""

model2 = SVC()
model2.fit(X_train,y_train)
y_pred2 = model2.predict(X_test)
accuracy(y_test,y_pred2),type2_error(cm(y_test,y_pred2))

C = [0.1, 1, 10, 100, 1000]
tol=[0.00001, 0.0001,0.001,0.01,0.1,1,10]
max_iter =[100,200,300,500]
opt = 1
x=0
y=0
z=0
for i in C:
  for j in tol :
    for k in max_iter :
      model1 = SVC(C=i,tol=j,max_iter=k)
      model1.fit(X_train,y_train)
      y_pred1 = model1.predict(X_validation)
      # print("i = ",i,"j = ",j,"k = ",k)
      # print(type2_error(cm(y_validation,y_pred1)))
      if(type2_error(cm(y_validation,y_pred1)) < opt):
        opt = type2_error(cm(y_validation,y_pred1))
        x=i
        y=j
        z=k

print(x,y,z)
print(opt)

model2 = SVC(C=1000,tol=0.01,max_iter=200)
model2.fit(X_train,y_train)
# y_pred2 = model2.predict_proba(X_test)
y_pred22 = model2.predict(X_test)
print(type2_error(cm(y_test,y_pred22)))

"""### **3. XGBoost Classifier**"""

model3 = XGBC()
model3.fit(X_train,y_train)
y_pred3 = model3.predict(X_test)
accuracy(y_test,y_pred3),type2_error(cm(y_test,y_pred3))

for i in range(1,10):
  model1 = XGBC(n_estimators = 100 + 50*i,eta = 0.1*i,subsample = 0.1*i,max_depth = 2+i,min_samples_split = 2+ 2*i,min_samples_leaf = 2+2*i)
  model1.fit(X_train,y_train)
  y_pred1 = model1.predict(X_validation)
  print("i = ",i)
  print(accuracy(y_validation,y_pred1),type2_error(cm(y_validation,y_pred1)))

min_samples_split = [2,3,5,7,10,15,20]
min_samples_leaf = [2,3,5,7,10,15,20]
for i in min_samples_leaf:
  for j in min_samples_split :
    model1 = XGBC(n_estimators = 450,eta = 0.7,max_depth = 9,subsample=0.7,min_samples_split = j,min_samples_leaf = i)
    model1.fit(X_train,y_train)
    y_pred1 = model1.predict(X_validation)
    print("i = ",i,"j = ",j)
    print(accuracy(y_validation,y_pred1),type2_error(cm(y_validation,y_pred1)))

n_estimators = [300,350,400,450,500,550,600,650,700]
max_depth = [2,3,5,7,10,15,20]
for i in max_depth:
  for j in n_estimators :
    model1 = XGBC(n_estimators = j,eta = 0.7,max_depth = i,subsample=0.7)
    model1.fit(X_train,y_train)
    y_pred1 = model1.predict(X_validation)
    print("i = ",i,"j = ",j)
    print(accuracy(y_validation,y_pred1),type2_error(cm(y_validation,y_pred1)))

subsample = [0.4,0.5,0.6,0.7,0.8,0.9,1]
eta = [0.01,0.1,0.2,0.5,0.7,0.8,0.9,1]
for i in eta:
  for j in subsample :
    model1 = XGBC(n_estimators = 700,eta = i,max_depth = 7,subsample=j)
    model1.fit(X_train,y_train)
    y_pred1 = model1.predict(X_validation)
    print("i = ",i,"j = ",j)
    print(accuracy(y_validation,y_pred1),type2_error(cm(y_validation,y_pred1)))

model3 = XGBC(n_estimators = 700,eta = 0.08 ,max_depth = 7)
model3.fit(X_train,y_train)
y_pred3 = model3.predict_proba(X_test)
y_pred33 = model3.predict(X_test)
# y_pred1 = pd.DataFrame(y_pred1)
    # print("i = ",i,"j = ",j)
print(roc_auc_score(y_test,y_pred3[:,1]),type2_error(cm(y_test,y_pred33)))

"""### **4. Multilayer Perceptron Classifier**"""

model4 = MLPC()
model4.fit(X_train,y_train)
y_pred4 = model4.predict(X_test)
type2_error(cm(y_test,y_pred4))

hidden_layers = [(64,32,16),(128,64,32,16),(32,32,32),(64,32),(128,32)]
# max_iter = ['lbfgs','adam','sgd']
max_iter = [100,200,300,400,500,600,700]
# learning_rate = [0.001,0.01,0.1,0.2,0.5,0.6,0.7,0.9]
for i in hidden_layers:
  for j in max_iter :
    model4 = MLPC(hidden_layer_sizes = i,max_iter=j)
    model4.fit(X_train,y_train)
    y_pred4 = model4.predict(X_validation)
    print("i = ",i,"j = ",j)
    print(type2_error(cm(y_validation,y_pred4)))

# hidden_layers = [(64,32,16),(128,64,32,16),(32,32,32),(64,32),(128,32)]
solver = ['lbfgs','adam','sgd']
# max_iter = [100,200,300,400,500,600,700]
learning_rate = [0.001,0.01,0.1,0.2]
for i in solver:
  for j in learning_rate :
    model4 = MLPC(hidden_layer_sizes = (64, 32, 16),max_iter=600,solver=i,learning_rate_init=j)
    model4.fit(X_train,y_train)
    y_pred4 = model4.predict(X_validation)
    print("i = ",i,"j = ",j)
    print(type2_error(cm(y_validation,y_pred4)))

# hidden_layers = [(64,32,16),(128,64,32,16),(32,32,32),(64,32),(128,32)]
# solver = ['lbfgs','adam','sgd']
# max_iter = [100,200,300,400,500,600,700]
# learning_rate = [0.001,0.01,0.1,0.2]
activation = ['relu','tanh','logistic']
for i in activation:
  # for j in learning_rate :
  model4 = MLPC(hidden_layer_sizes = (128,64, 32, 16),max_iter=600,solver='adam',learning_rate_init=0.01,activation = i)
  model4.fit(X_train,y_train)
  y_pred4 = model4.predict(X_validation)
  print("i = ",i)
  print(type2_error(cm(y_validation,y_pred4)))

model4 = MLPC(hidden_layer_sizes = (128,64, 32, 16),max_iter=600,solver='adam',learning_rate_init=0.01,activation = 'tanh')
model4.fit(X_train,y_train)
y_pred4 = model4.predict_proba(X_test)
y_pred44 = model4.predict(X_test)
print(roc_auc_score(y_test,y_pred4[:,1]),type2_error(cm(y_test,y_pred44)))

"""### **5. KNN Classifier**"""

acc_list=[]
best_params=[]

n_neighbors = [5,7,9,11,13,15]
weights = ['uniform','distance']
metric = ['minkowski','euclidean','manhattan']

for i in n_neighbors:
  for j in weights:
    for k in metric:
      params_list=[]
      knn_classifier = KNC(weights =  j, n_neighbors = i, metric = k) # Hyperparameters 
      knn_classifier.fit(X_train, y_train)                      
      knn_predictions = knn_classifier.predict_proba(X_validation)
      acc_list.append(roc_auc_score(y_validation , knn_predictions[:,1]))
      params_list.append(i)
      params_list.append(j)
      params_list.append(k)
      best_params.append(params_list)

max(acc_list)

best_params[acc_list.index(max(acc_list))]

model6 = KNC(weights =  'distance', n_neighbors = 15, metric = 'minkowski') 
model6.fit(X_train, y_train)    
ypred6 = knn_classifier.predict_proba(X_test)

print(roc_auc_score(y_test,ypred6[:,1]))

type2_error(cm(y_test, model6.predict(X_test)))



"""##**Applying RandomizedSearchCV with cross validation to find out the optimal hyperparameters for the models**

### **1. Random Forest Classifier Model**
"""

# Finding the optimal hyperparameters using RandomizedSearchCV along with cross validation


# Parameter grid for Random Forest Classifier
parameters_for_rfc = {
        'n_estimators': [100,200,500,700,1000],
        'max_depth': [3,5,7,9,11],
        'min_samples_split': [20,40,60,80,100],
        'min_samples_leaf': [20,40,60,80,100],
}
print("*************************************************************************************\n")


# Finding the best hyperparameters using RandomizedSearchCV
rfc_randomizedsearch = RandomizedSearchCV(estimator = RFC(), scoring='accuracy', param_distributions = parameters_for_rfc, return_train_score=True, verbose=1, cv=5)
rfc_randomizedsearch.fit(X_train, y_train) 
best_parameters_for_rfc = rfc_randomizedsearch.best_params_ 
print("*************************************************************************************\n")
print("Best parameters for Random Forest Classifier are: ")
print(best_parameters_for_rfc)
print("\n*************************************************************************************\n")

"""### **2. Support Vector Classifier Model**

"""

# Finding the optimal hyperparameters using RandomizedSearchCV


# Parameter grid for LinearSVC
parameters_for_svc = {
        'C': [0.1, 1, 10, 100, 1000],
        'tol':[0.00001, 0.0001,0.001,0.01,0.1,1,10],
        'max_iter':[100,200,300,500]
}
print("*************************************************************************************\n")

# Finding the best hyperparameters using RandomizedSearchCV
svc_randomizedsearch = RandomizedSearchCV(estimator = SVC(), scoring='accuracy', param_distributions = parameters_for_svc, return_train_score=True, verbose=1, cv=5)
svc_randomizedsearch.fit(X_train, y_train) 
best_parameters_for_svc = svc_randomizedsearch.best_params_ 
print("*************************************************************************************\n")
print("Best parameters for Linear Support Vector Classifier are: ")
print(best_parameters_for_svc)
print("\n*************************************************************************************\n")

"""### **3. XGBoost Classifier Model**

"""

# Finding the optimal hyperparameters using RandomizedSearchCV along with cross validation
# Parameter grid for XGBoost
parameters_for_xgboost = {
    'learning_rate' : [0.05,0.10,0.15,0.20,0.25,0.30],
    'max_depth' : [ 3, 4, 5, 6, 8, 10, 12, 15],
    'min_child_weight' : [ 1, 3, 5, 7 ],
    'gamma': [ 0.0, 0.1, 0.2 , 0.3, 0.4 ],
    'colsample_bytree' : [ 0.3, 0.4, 0.5 , 0.7],
    'reg_lambda': [0.1, 1.0, 5.0, 10.0, 50.0, 100.0],
}
print("*************************************************************************************\n")

# Finding the best hyperparameters using RandomizedSearchCV
xgb_randomizedsearch = RandomizedSearchCV(XGBC(), parameters_for_xgboost, return_train_score=True, verbose=1, cv=5)
xgb_randomizedsearch.fit(X_train, y_train) 
best_parameters_for_xgboost = xgb_randomizedsearch.best_params_ 
print("*************************************************************************************\n")
print("Best parameters for XGBoost Classifier are: ")
print(best_parameters_for_xgboost)
print("\n*************************************************************************************\n")

"""### **4. KNN Classifier Model**"""

# Finding the optimal hyperparameters using RandomizedSearchCV along with cross validation
# Parameter grid for KNN
parameters_for_knn = {
    'n_neighbors' : [5,7,9,11,13,15],
    'weights' : ['uniform','distance'],
    'metric' : ['minkowski','euclidean','manhattan']
}
print("*************************************************************************************\n")


# Finding the best hyperparameters using RandomizedSearchCV
knn_randomizedsearch = RandomizedSearchCV(KNC(), parameters_for_knn, return_train_score=True, verbose=1, cv=5)
knn_randomizedsearch.fit(X_train, y_train) 
best_parameters_for_knn = knn_randomizedsearch.best_params_ 
print("*************************************************************************************\n")
print("Best parameters for KNN Classifier are: ")
print(best_parameters_for_knn)
print("\n*************************************************************************************\n")

"""### **5. MLP Classifier Model**

"""

# Finding the optimal hyperparameters using RandomizedSearchCV along with cross validation
# Parameter grid for MLP
parameters_for_mlp = {
    'hidden_layer_sizes' : [(64,32,16),(128,64,32,16),(32,32,32),(64,32),(128,32)],
    'max_iter': [100,200,300,400,500,600,700],
    'learning_rate_init':[0.001,0.01,0.1,0.2,0.5,0.6,0.7,0.9, 1, 2, 3, 5],
}
print("*************************************************************************************\n")


# Finding the best hyperparameters using RandomizedSearchCV
mlp_randomizedsearch = RandomizedSearchCV(MLPC(), parameters_for_mlp, return_train_score=True, verbose=1, cv=5)
mlp_randomizedsearch.fit(X_train, y_train) 
best_parameters_for_mlp = mlp_randomizedsearch.best_params_ 
print("*************************************************************************************\n")
print("Best parameters for MLP Classifier are: ")
print(best_parameters_for_mlp)
print("\n*************************************************************************************\n")

"""## **Evaluation of the models with best hyperparameters on Validation Data**"""

# Evaluating the performance using Random Forest Classifier

rfc_classifier = RFC(n_estimators = 1000, min_samples_split = 80, min_samples_leaf = 20, max_depth = 7) # Hyperparameters 
rfc_classifier.fit(X_train, y_train)                      
rfc_predictions = rfc_classifier.predict(X_validation)  
print("\n*************************************************************************************\n")

# Evaluating the performance on the test dataset
print("Accuracy obtained by Random Forest Classifier is :", accuracy(y_validation, rfc_predictions)) 
print("F1 Score obtained by Random Forest Classifier is :", f1score(y_validation, rfc_predictions)) 
print("Precision obtained by Random Forest Classifier is :", precision(y_validation, rfc_predictions))  
print("Recall obtained by Random Forest Classifier is :", recall(y_validation, rfc_predictions)) 
print("AUC score obtained by Random Forest Classifier is :", roc_auc_score(y_validation, rfc_predictions)) 
print("\n*************************************************************************************\n") 


# Calculating the Type-1 Error and Type-2 Errors
print("Type-1 Error obtained by Random Forest Classifier is :", cm(y_validation, rfc_predictions)[1][0]/(cm(y_validation, rfc_predictions)[1][0] + cm(y_validation, rfc_predictions)[1][1]))
print("Type-2 Error obtained by Random Forest Classifier is :", cm(y_validation, rfc_predictions)[0][1]/(cm(y_validation, rfc_predictions)[1][1] + cm(y_validation, rfc_predictions)[0][1]))
print("\n*************************************************************************************\n")


# Plotting ROC curve for the model
print("ROC Curve for Random Forest Classifier:\n")
plot_roc_curve(rfc_classifier, X_validation, y_validation) 
print("\n*************************************************************************************\n")

# Evaluating the performance using LinearSVC Classifier

svc_classifier = SVC(tol = 1e-05, max_iter = 300, C = 0.1) # Hyperparameters 
svc_classifier.fit(X_train, y_train)                      
svc_predictions = svc_classifier.predict(X_validation)  
print("*************************************************************************************\n")

# Evaluating the performance on the test dataset
print("Accuracy obtained by Linear Support Vector Classifier is :", accuracy(y_validation, svc_predictions)) 
print("F1 Score obtained by Linear Support Vector Classifier is :", f1score(y_validation, svc_predictions)) 
print("Precision obtained by Linear Support Vector Classifier is :", precision(y_validation, svc_predictions)) 
print("Recall obtained by Linear Support Vector Classifier is :", recall(y_validation, svc_predictions)) 
print("AUC score obtained by Linear Support Vector Classifier is :", roc_auc_score(y_validation, svc_predictions))
print("\n*************************************************************************************\n")


# Calculating the Type-1 Error and Type-2 Errors
print("Type-1 Error obtained by Linear Support Vector Classifier is :", cm(y_validation, svc_predictions)[1][0] / (cm(y_validation, svc_predictions)[1][0] + cm(y_validation, svc_predictions)[1][1]))
print("Type-2 Error obtained by Linear Support Vector Classifier is :", cm(y_validation, svc_predictions)[0][1] / (cm(y_validation, svc_predictions)[0][1] + cm(y_validation, svc_predictions)[1][1]))
print("\n*************************************************************************************\n")


# Plotting ROC curve for the model
print("ROC Curve for Linear Support Vector Classifier:\n")
plot_roc_curve(svc_classifier, X_validation, y_validation) 
print("\n*************************************************************************************\n")

# Evaluating the performance using XGB Classifier

xgb_classifier = XGBC(reg_lambda = 1.0, min_child_weight = 1, max_depth = 10, learning_rate = 0.15, gamma = 0.2, colsample_bytree = 0.7) # Hyperparameters 
xgb_classifier.fit(X_train, y_train)                      
xgb_predictions = xgb_classifier.predict(X_validation)  
print("\n*************************************************************************************\n")

# Evaluating the performance on the test dataset
print("Accuracy obtained by XGBoost Classifier is :", accuracy(y_validation, xgb_predictions)) 
print("F1 Score obtained by XGBoost Classifier is :", f1score(y_validation, xgb_predictions)) 
print("Precision obtained by XGBoost Classifier is :", precision(y_validation, xgb_predictions)) 
print("Recall obtained by XGBoost Classifier is :", recall(y_validation, xgb_predictions)) 
print("AUC score obtained by XGBoost Classifier is :", roc_auc_score(y_validation, xgb_predictions))
print("\n*************************************************************************************\n")


# Calculating the Type-1 Error and Type-2 Errors
print("Type-1 Error obtained by XGBoost Classifier is :", cm(y_validation, xgb_predictions)[1][0] / (cm(y_validation, xgb_predictions)[1][0] + cm(y_validation, xgb_predictions)[1][1]))
print("Type-2 Error obtained by XGBoost Classifier is :", cm(y_validation, xgb_predictions)[0][1] / (cm(y_validation, xgb_predictions)[0][1] + cm(y_validation, xgb_predictions)[1][1]))
print("\n*************************************************************************************\n")


# Plotting ROC curve for the model
print("ROC Curve for XGBoost Classifier:", '\n')
plot_roc_curve(xgb_classifier, X_validation, y_validation) 
print("\n*************************************************************************************\n")

# Evaluating the performance using KNN Classifier

knn_classifier = KNC(weights =  'distance', n_neighbors = 5, metric = 'minkowski') # Hyperparameters 
knn_classifier.fit(X_train, y_train)                      
knn_predictions = knn_classifier.predict(X_validation)  
print("\n*************************************************************************************\n")

# Evaluating the performance on the test dataset
print("Accuracy obtained by KNN Classifier is :", accuracy(y_validation, knn_predictions)) 
print("F1 Score obtained by KNN Classifier is :", f1score(y_validation, knn_predictions)) 
print("Precision obtained by KNN Classifier is :", precision(y_validation, knn_predictions))  
print("Recall obtained by KNN Classifier is :", recall(y_validation, knn_predictions)) 
print("AUC score obtained by KNN Classifier is :", roc_auc_score(y_validation, knn_predictions)) 
print("\n*************************************************************************************\n") 


# Calculating the Type-1 Error and Type-2 Errors
print("Type-1 Error obtained by KNN Classifier is :", cm(y_validation, knn_predictions)[1][0]/(cm(y_validation, rfc_predictions)[1][0] + cm(y_validation, knn_predictions)[1][1]))
print("Type-2 Error obtained by KNN Classifier is :", cm(y_validation, knn_predictions)[0][1]/(cm(y_validation, rfc_predictions)[1][1] + cm(y_validation, knn_predictions)[0][1]))
print("\n*************************************************************************************\n")


# Plotting ROC curve for the model
print("ROC Curve for KNN Classifier:\n")
plot_roc_curve(knn_classifier, X_validation, y_validation) 
print("\n*************************************************************************************\n")

# Evaluating the performance using MLP Classifier

mlp_classifier = MLPC(max_iter = 500, learning_rate_init = 0.001, hidden_layer_sizes = (128, 64, 32, 16)) # Hyperparameters 
mlp_classifier.fit(X_train, y_train)                      
mlp_predictions = mlp_classifier.predict(X_validation)  
print("\n*************************************************************************************\n")

# Evaluating the performance on the test dataset
print("Accuracy obtained by MLP Classifier is :", accuracy(y_validation, mlp_predictions)) 
print("F1 Score obtained by MLP Classifier is :", f1score(y_validation, mlp_predictions)) 
print("Precision obtained by MLP Classifier is :", precision(y_validation, mlp_predictions))  
print("Recall obtained by MLP Classifier is :", recall(y_validation, mlp_predictions)) 
print("AUC score obtained by MLP Classifier is :", roc_auc_score(y_validation, mlp_predictions)) 
print("\n*************************************************************************************\n") 


# Calculating the Type-1 Error and Type-2 Errors
print("Type-1 Error obtained by MLP Classifier is :", cm(y_validation, mlp_predictions)[1][0]/(cm(y_validation, mlp_predictions)[1][0] + cm(y_validation, mlp_predictions)[1][1]))
print("Type-2 Error obtained by MLP Classifier is :", cm(y_validation, mlp_predictions)[0][1]/(cm(y_validation, mlp_predictions)[1][1] + cm(y_validation, mlp_predictions)[0][1]))
print("\n*************************************************************************************\n")


# Plotting ROC curve for the model
print("ROC Curve for MLP Classifier:\n")
plot_roc_curve(mlp_classifier, X_validation, y_validation) 
print("\n*************************************************************************************\n")

"""## **Making Predictions on Unseen Data**"""

# Using Min-Max Scaler to scale the data
scaler = MMS()
X_test['bmi'] = scaler.fit_transform(pd.DataFrame(X_test['bmi']))
X_test['avg_glucose_level'] = scaler.fit_transform(pd.DataFrame(X_test['avg_glucose_level']))
X_test['age'] = scaler.fit_transform(pd.DataFrame(X_test['age']))

rfc_test_predictions = rfc_classifier.predict(X_test)  

print("\n*************************************************************************************\n")

# Evaluating the performance on the test dataset
print("Accuracy obtained by Random Forest Classifier is :", accuracy(y_test, rfc_test_predictions)) 
print("F1 Score obtained by Random Forest Classifier is :", f1score(y_test, rfc_test_predictions)) 
print("Precision obtained by Random Forest Classifier is :", precision(y_test, rfc_test_predictions))  
print("Recall obtained by Random Forest Classifier is :", recall(y_test, rfc_test_predictions)) 
print("AUC score obtained by Random Forest Classifier is :", roc_auc_score(y_test, rfc_test_predictions)) 
print("\n*************************************************************************************\n") 


# Calculating the Type-1 Error and Type-2 Errors
print("Type-1 Error obtained by Random Forest Classifier is :", cm(y_test, rfc_test_predictions)[1][0]/(cm(y_test, rfc_test_predictions)[1][0] + cm(y_test, rfc_test_predictions)[1][1]))
print("Type-2 Error obtained by Random Forest Classifier is :", cm(y_test, rfc_test_predictions)[0][1]/(cm(y_test, rfc_test_predictions)[1][1] + cm(y_test, rfc_test_predictions)[0][1]))
print("\n*************************************************************************************\n")


# Plotting ROC curve for the model
print("ROC Curve for Random Forest Classifier:\n")
plot_roc_curve(rfc_classifier, X_test, y_test) 
print("\n*************************************************************************************\n")

svc_test_predictions = svc_classifier.predict(X_test)  
print("*************************************************************************************\n")

# Evaluating the performance on the test dataset
print("Accuracy obtained by Linear Support Vector Classifier is :", accuracy(y_test, svc_test_predictions)) 
print("F1 Score obtained by Linear Support Vector Classifier is :", f1score(y_test, svc_test_predictions)) 
print("Precision obtained by Linear Support Vector Classifier is :", precision(y_test, svc_test_predictions)) 
print("Recall obtained by Linear Support Vector Classifier is :", recall(y_test, svc_test_predictions)) 
print("AUC score obtained by Linear Support Vector Classifier is :", roc_auc_score(y_test, svc_test_predictions))
print("\n*************************************************************************************\n")


# Calculating the Type-1 Error and Type-2 Errors
print("Type-1 Error obtained by Linear Support Vector Classifier is :", cm(y_test, svc_test_predictions)[1][0] / (cm(y_test, svc_test_predictions)[1][0] + cm(y_test, svc_test_predictions)[1][1]))
print("Type-2 Error obtained by Linear Support Vector Classifier is :", cm(y_test, svc_test_predictions)[0][1] / (cm(y_test, svc_test_predictions)[0][1] + cm(y_test, svc_test_predictions)[1][1]))
print("\n*************************************************************************************\n")


# Plotting ROC curve for the model
print("ROC Curve for Linear Support Vector Classifier:\n")
plot_roc_curve(svc_classifier, X_test, y_test) 
print("\n*************************************************************************************\n")

xgb_test_predictions = xgb_classifier.predict(X_test)  
print("\n*************************************************************************************\n")

# Evaluating the performance on the test dataset
print("Accuracy obtained by XGBoost Classifier is :", accuracy(y_test, xgb_test_predictions)) 
print("F1 Score obtained by XGBoost Classifier is :", f1score(y_test, xgb_test_predictions)) 
print("Precision obtained by XGBoost Classifier is :", precision(y_test, xgb_test_predictions)) 
print("Recall obtained by XGBoost Classifier is :", recall(y_test, xgb_test_predictions)) 
print("AUC score obtained by XGBoost Classifier is :", roc_auc_score(y_test, xgb_test_predictions))
print("\n*************************************************************************************\n")


# Calculating the Type-1 Error and Type-2 Errors
print("Type-1 Error obtained by XGBoost Classifier is :", cm(y_test, xgb_test_predictions)[1][0] / (cm(y_test, xgb_test_predictions)[1][0] + cm(y_test, xgb_test_predictions)[1][1]))
print("Type-2 Error obtained by XGBoost Classifier is :", cm(y_test, xgb_test_predictions)[0][1] / (cm(y_test, xgb_test_predictions)[0][1] + cm(y_test, xgb_test_predictions)[1][1]))
print("\n*************************************************************************************\n")


# Plotting ROC curve for the model
print("ROC Curve for XGBoost Classifier:", '\n')
plot_roc_curve(xgb_classifier, X_test, y_test) 
print("\n*************************************************************************************\n")

knn_test_predictions = knn_classifier.predict(X_test)  
print("\n*************************************************************************************\n")

# Evaluating the performance on the test dataset
print("Accuracy obtained by KNN Classifier is :", accuracy(y_test, knn_test_predictions)) 
print("F1 Score obtained by KNN Classifier is :", f1score(y_test, knn_test_predictions)) 
print("Precision obtained by KNN Classifier is :", precision(y_test, knn_test_predictions))  
print("Recall obtained by KNN Classifier is :", recall(y_test, knn_test_predictions)) 
print("AUC score obtained by KNN Classifier is :", roc_auc_score(y_test, knn_test_predictions)) 
print("\n*************************************************************************************\n") 


# Calculating the Type-1 Error and Type-2 Errors
print("Type-1 Error obtained by KNN Classifier is :", cm(y_test, knn_test_predictions)[1][0]/(cm(y_test, knn_test_predictions)[1][0] + cm(y_test, knn_test_predictions)[1][1]))
print("Type-2 Error obtained by KNN Classifier is :", cm(y_test, knn_test_predictions)[0][1]/(cm(y_test, knn_test_predictions)[1][1] + cm(y_test, knn_test_predictions)[0][1]))
print("\n*************************************************************************************\n")


# Plotting ROC curve for the model
print("ROC Curve for KNN Classifier:\n")
plot_roc_curve(knn_classifier, X_test, y_test) 
print("\n*************************************************************************************\n")

mlp_test_predictions = mlp_classifier.predict(X_test)  
print("\n*************************************************************************************\n")

# Evaluating the performance on the test dataset
print("Accuracy obtained by MLP Classifier is :", accuracy(y_test, mlp_test_predictions)) 
print("F1 Score obtained by MLP Classifier is :", f1score(y_test, mlp_test_predictions)) 
print("Precision obtained by MLP Classifier is :", precision(y_test, mlp_test_predictions))  
print("Recall obtained by MLP Classifier is :", recall(y_test, mlp_test_predictions)) 
print("AUC score obtained by MLP Classifier is :", roc_auc_score(y_test, mlp_test_predictions)) 
print("\n*************************************************************************************\n") 


# Calculating the Type-1 Error and Type-2 Errors
print("Type-1 Error obtained by MLP Classifier is :", cm(y_test, mlp_test_predictions)[1][0]/(cm(y_test, mlp_test_predictions)[1][0] + cm(y_test, mlp_test_predictions)[1][1]))
print("Type-2 Error obtained by MLP Classifier is :", cm(y_test, mlp_test_predictions)[0][1]/(cm(y_test, mlp_test_predictions)[1][1] + cm(y_test, mlp_test_predictions)[0][1]))
print("\n*************************************************************************************\n")


# Plotting ROC curve for the model
print("ROC Curve for MLP Classifier:\n")
plot_roc_curve(mlp_classifier, X_test, y_test) 
print("\n*************************************************************************************\n")

"""## **Comparision of ROC scores and Type 2 errors**

"""

models = ['RFC','SVC','XGBoost','MLP','KNN']
roc_auc_scores = [0.9524490959485625,0,0.96053089618603,0.9422828686533407,0.947324820629195]
type2_errors = [0.15044814340588988,0.0851528384279476,0.14702233250620347,0.15958102279728897,0.1873508353221957]
plt.figure(figsize=(5,5))
sns.set(rc={'figure.figsize':(11,4)})  
sns.barplot(x=models, y=roc_auc_scores)
plt.title("roc auc scores")
plt.show()
plt.figure(figsize=(5,5))

sns.barplot(x=models, y=type2_errors)
plt.title("type2 errors")
plt.show()

"""## **Deploying the model using Flask and Pickle**"""

# Pickle library for integration with frontend
import pickle

# Saving the models 
saved_data = {"model":xgb_classifier}
with open('saved_steps.pkl', 'wb') as file:
    pickle.dump(saved_data, file)

# Loading the models
with open('saved_steps.pkl', 'rb') as file:
    data = pickle.load(file)

# Save the pkl file
from google.colab import files
files.download('saved_steps.pkl')